import pandas as pd

# Load the CSV file
data = pd.read_csv('ecom_txns.csv')

# Display the first few rows of the dataset
data.head()


import geoip2.database


# Initialize the reader
reader = geoip2.database.Reader('GeoLite2-Country.mmdb')

# Function to fetch country from IP address
def get_country_from_ip(ip):
    try:
        response = reader.country(ip)
        return response.country.name
    except:
        return "Unknown"

# Apply the function to the dataset
data['Country'] = data['ip_address'].apply(get_country_from_ip)

# Close the reader
reader.close()



# Display the updated dataset
data.head()


from IPython.display import FileLink

# Save the DataFrame as a CSV file
data.to_csv('updated_file.csv', index=False)

# Create a link to download the file
FileLink(r'updated_file.csv')


# Analyze missing values
missing_values = data.isnull().sum()
missing_percentage = (missing_values / len(data)) * 100

# Create a DataFrame to summarize missing value information
missing_summary = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage (%)': missing_percentage
}).sort_values(by='Missing Values', ascending=False)

# Display the summary in tabular format
from IPython.display import display
display(missing_summary)



# Replace missing values in 'Country' with 'Unknown'
#data['Country'].fillna('Unknown', inplace=True)
data.fillna({'Country':'Unknown'}, inplace=True)

# Verify if missing values in 'Country' have been handled
missing_values_after = data['Country'].isnull().sum()
missing_values_after


import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# 1. Date-Time Features
data['signup_datetime'] = pd.to_datetime(data['signup_datetime'])
data['datetime'] = pd.to_datetime(data['datetime'])

# Extracting transaction hour, day of week, and time difference
data['transaction_hour'] = data['datetime'].dt.hour
data['transaction_day'] = data['datetime'].dt.dayofweek
data['time_since_signup'] = (data['datetime'] - data['signup_datetime']).dt.total_seconds() / 3600  # Hours

# 2. Categorical Encoding
# One-hot encode 'store' and 'browser'
encoder = OneHotEncoder(drop='first', sparse_output=False)
categorical_encoded = pd.DataFrame(
    encoder.fit_transform(data[['store', 'browser']]),
    columns=encoder.get_feature_names_out(['store', 'browser'])
)

data = pd.concat([data, categorical_encoded], axis=1)

# Frequency encoding for 'Country'
country_frequency = data['Country'].value_counts(normalize=True)
data['country_frequency'] = data['Country'].map(country_frequency)

# Encode 'sex' as numeric
data['sex'] = data['sex'].astype('category').cat.codes

# 3. Numerical Feature Transformation
scaler = StandardScaler()
data[['amount', 'age', 'time_since_signup']] = scaler.fit_transform(data[['amount', 'age', 'time_since_signup']])

# 4. Interaction Features
data['device_browser'] = data['device_id'].astype(str) + "_" + data['browser']
data['country_store'] = data['Country'].astype(str) + "_" + data['store']

# Encoding interaction features (frequency encoding for simplicity)
device_browser_freq = data['device_browser'].value_counts(normalize=True)
country_store_freq = data['country_store'].value_counts(normalize=True)
data['device_browser_freq'] = data['device_browser'].map(device_browser_freq)
data['country_store_freq'] = data['country_store'].map(country_store_freq)

# 5. Aggregated User Behavior
user_aggregates = data.groupby('user_id').agg({
    'amount': ['mean', 'std'],
    'fraud': 'sum'
}).reset_index()

user_aggregates.columns = ['user_id', 'user_avg_amount', 'user_std_amount', 'user_fraud_history']

data = data.merge(user_aggregates, on='user_id', how='left')

# Dropping unnecessary columns for modeling
columns_to_drop = [
    'signup_datetime', 'datetime', 'ip_address', 'store', 'browser', 'Country',
    'device_id', 'device_browser', 'country_store'
]
data_model = data.drop(columns=columns_to_drop, errors='ignore')

# Display the summary in tabular format
from IPython.display import display
display(data_model)


#6. numerical feature normalization
# Select numerical columns to normalize
numerical_columns = ['amount', 'age']

# Normalize numerical columns using StandardScaler
scaler = StandardScaler()
data_model[numerical_columns] = scaler.fit_transform(data_model[numerical_columns])

# Verify normalization by inspecting the first few rows
data_model[numerical_columns].head()


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score

# Handle Missing Values
if data_model.isnull().sum().any():
    # Fill missing numerical values with the median
    for col in data_model.select_dtypes(include=np.number).columns:
        data_model[col].fillna(data_model[col].median(), inplace=True)
    
    # Fill missing categorical values with the mode
    for col in data_model.select_dtypes(include='object').columns:
        data_model[col].fillna(data_model[col].mode()[0], inplace=True)


# Define features (X) and target (y)
X = data_model.drop(columns=['fraud', 'user_id'])
y = data_model['fraud']

# Split the data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
    report = classification_report(y_test, y_pred, output_dict=True)
    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else None
    results[name] = {"classification_report": report, "roc_auc": auc}

results





# Prepare a detailed comparison table for metrics (precision, recall, F1-score, support, ROC-AUC)
metrics_comparison = []

for model_name, result in results.items():
    report = result["classification_report"]
    auc = result["roc_auc"]
    for label in report.keys():
        if label in ['0', '1']:  # Only consider the fraud (1) and non-fraud (0) classes
            metrics_comparison.append({
                "Model": model_name,
                "Class": label,
                "Precision": report[label]['precision'],
                "Recall": report[label]['recall'],
                "F1-Score": report[label]['f1-score'],
                "Support": report[label]['support'],
                "ROC-AUC": auc if label == '1' else None  # ROC-AUC is a single score, include it only for 'fraud'
            })

# Convert the metrics to a DataFrame for tabular presentation
comparison_df = pd.DataFrame(metrics_comparison)

# Display the comparison table to the user
comparison_df



import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc
import numpy as np

# Train and evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
    report = classification_report(y_test, y_pred, output_dict=True)
    auc_score = roc_auc_score(y_test, y_proba) if y_proba is not None else None
    results[name] = {"classification_report": report, "roc_auc": auc_score, "proba": y_proba}

# Prepare data for plotting
metrics = ["Precision", "Recall", "F1-Score"]
classes = ["0 (Non-Fraud)", "1 (Fraud)"]
models_list = list(models.keys())

# Aggregate metrics for each model and class
plot_data = {metric: {cls: [] for cls in classes} for metric in metrics}
for model_name, result in results.items():
    report = result["classification_report"]
    for metric in metrics:
        for cls in classes:
            cls_label = cls.split()[0]  # Extract class label ("0" or "1")
            plot_data[metric][cls].append(report[cls_label][metric.lower()])

# Create bar plots for each metric
for metric in metrics:
    x = np.arange(len(models_list))  # model indices
    width = 0.35  # bar width
    
    fig, ax = plt.subplots(figsize=(10, 6))
    for idx, cls in enumerate(classes):
        ax.bar(x + idx * width, plot_data[metric][cls], width, label=cls)
    
    # Add labels and title
    ax.set_xlabel("Models")
    ax.set_ylabel(metric)
    ax.set_title(f"{metric} Comparison Across Models")
    ax.set_xticks(x + width / 2)
    ax.set_xticklabels(models_list)
    ax.legend(title="Class")
    
    plt.show()

# Generate ROC curves for all models
plt.figure(figsize=(10, 6))
for model_name, result in results.items():
    if result["proba"] is not None:
        y_proba = result["proba"]
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{model_name} (AUC = {roc_auc:.2f})")

# Add plot details
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves")
plt.legend(loc="lower right")
plt.show()


# Evaluate training and test performance for overfitting check
overfitting_check = []

for name, model in models.items():
    y_train_pred = model.predict(X_train)
    y_train_proba = model.predict_proba(X_train)[:, 1] if hasattr(model, "predict_proba") else None
    y_test_pred = model.predict(X_test)
    y_test_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
    
    # Metrics for training and test data
    train_auc = roc_auc_score(y_train, y_train_proba) if y_train_proba is not None else None
    test_auc = roc_auc_score(y_test, y_test_proba) if y_test_proba is not None else None
    
    train_report = classification_report(y_train, y_train_pred, output_dict=True)
    test_report = classification_report(y_test, y_test_pred, output_dict=True)
    
    # Append results
    overfitting_check.append({
        "Model": name,
        "Train ROC-AUC": train_auc,
        "Test ROC-AUC": test_auc,
        "Train F1-Score (Fraud)": train_report['1']['f1-score'],
        "Test F1-Score (Fraud)": test_report['1']['f1-score']
    })

# Convert the overfitting check results to a DataFrame for easier comparison
overfitting_df = pd.DataFrame(overfitting_check)

# Display the comparison table to the user
overfitting_df


from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation for each model and calculate ROC-AUC
cv_results = []

for name, model in models.items():
    if hasattr(model, "predict_proba"):
        auc_scores = cross_val_score(
            model, X, y, cv=5, scoring='roc_auc'
        )  # Use ROC-AUC as the scoring metric
        cv_results.append({
            "Model": name,
            "Mean ROC-AUC": auc_scores.mean(),
            "Std ROC-AUC": auc_scores.std()
        })

# Convert the cross-validation results to a DataFrame for easier interpretation
cv_results_df = pd.DataFrame(cv_results)

# Display the cross-validation summary to the user
cv_results_df


# Calculate the correlation matrix for numeric features
correlation_matrix = data_model.corr()

# Extract correlations with the target variable 'fraud'
fraud_correlations = correlation_matrix['fraud'].sort_values(ascending=False)

# Filter features with high correlation with 'fraud' (absolute correlation > 0.5 as a threshold)
highly_correlated_features = fraud_correlations[abs(fraud_correlations) > 0.5]

# Display the results to identify potential leakage
highly_correlated_features



# Remove the 'user_fraud_history' feature from the dataset
X_cleaned = X.drop(columns=['user_fraud_history'])

# Re-initialize models
models_cleaned = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

# Re-train models on the cleaned data
results_cleaned = {}

for name, model in models_cleaned.items():
    model.fit(X_train[X_cleaned.columns], y_train)
    y_pred = model.predict(X_test[X_cleaned.columns])
    y_proba = model.predict_proba(X_test[X_cleaned.columns])[:, 1] if hasattr(model, "predict_proba") else None
    report = classification_report(y_test, y_pred, output_dict=True)
    auc_score = roc_auc_score(y_test, y_proba) if y_proba is not None else None
    results_cleaned[name] = {"classification_report": report, "roc_auc": auc_score}

# Summarize the re-trained results
summary_cleaned = []

for name, result in results_cleaned.items():
    report = result["classification_report"]
    auc = result["roc_auc"]
    summary_cleaned.append({
        "Model": name,
        "ROC-AUC": auc,
        "Precision (Fraud)": report['1']['precision'],
        "Recall (Fraud)": report['1']['recall'],
        "F1-Score (Fraud)": report['1']['f1-score']
    })

# Convert the summary to a DataFrame
summary_cleaned_df = pd.DataFrame(summary_cleaned)

# Display the results to the user
summary_cleaned_df


import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc
import numpy as np

# Train and evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train[X_cleaned.columns], y_train)
    y_pred = model.predict(X_test[X_cleaned.columns])
    y_proba = model.predict_proba(X_test[X_cleaned.columns])[:, 1] if hasattr(model, "predict_proba") else None
    report = classification_report(y_test, y_pred, output_dict=True)
    auc_score = roc_auc_score(y_test, y_proba) if y_proba is not None else None
    results[name] = {"classification_report": report, "roc_auc": auc_score, "proba": y_proba}

# Prepare data for plotting
metrics = ["Precision", "Recall", "F1-Score"]
classes = ["0 (Non-Fraud)", "1 (Fraud)"]
models_list = list(models.keys())

# Aggregate metrics for each model and class
plot_data = {metric: {cls: [] for cls in classes} for metric in metrics}
for model_name, result in results.items():
    report = result["classification_report"]
    for metric in metrics:
        for cls in classes:
            cls_label = cls.split()[0]  # Extract class label ("0" or "1")
            plot_data[metric][cls].append(report[cls_label][metric.lower()])

# Create bar plots for each metric
for metric in metrics:
    x = np.arange(len(models_list))  # model indices
    width = 0.35  # bar width
    
    fig, ax = plt.subplots(figsize=(10, 6))
    for idx, cls in enumerate(classes):
        ax.bar(x + idx * width, plot_data[metric][cls], width, label=cls)
    
    # Add labels and title
    ax.set_xlabel("Models")
    ax.set_ylabel(metric)
    ax.set_title(f"{metric} Comparison Across Models")
    ax.set_xticks(x + width / 2)
    ax.set_xticklabels(models_list)
    ax.legend(title="Class")
    
    plt.show()

# Generate ROC curves for all models
plt.figure(figsize=(10, 6))
for model_name, result in results.items():
    if result["proba"] is not None:
        y_proba = result["proba"]
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{model_name} (AUC = {roc_auc:.2f})")

# Add plot details
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves")
plt.legend(loc="lower right")
plt.show()



