

















import pandas as pd
import numpy as np

# Load the CSV file
data = pd.read_csv('ecom_txns.csv')

# Display the first few rows of the dataset
data.head()





import geoip2.database


# Initialize the reader
reader = geoip2.database.Reader('GeoLite2-Country.mmdb')

# Function to fetch country from IP address
def get_country_from_ip(ip):
    try:
        response = reader.country(ip)
        return response.country.name
    except:
        return "Unknown"

# Apply the function to the dataset
data['Country'] = data['ip_address'].apply(get_country_from_ip)

# Close the reader
reader.close()



# Display the updated dataset
data.head()


from IPython.display import FileLink

# Save the DataFrame as a CSV file
data.to_csv('updated_file.csv', index=False)

# Create a link to download the file
FileLink(r'updated_file.csv')





import matplotlib.pyplot as plt

# Visualize the fraud distribution
fraud_counts = data['fraud'].value_counts()
plt.figure(figsize=(6, 4))
fraud_counts.plot(kind='bar', color=['blue', 'orange'])
plt.title('Fraud Distribution')
plt.xticks(ticks=[0, 1], labels=['Legitimate', 'Fraud'])
plt.ylabel('Count')
plt.xlabel('Transaction Type')
plt.show()

# Visualize transaction amounts for fraudulent vs. legitimate transactions
plt.figure(figsize=(10, 6))
data.boxplot(column='amount', by='fraud', showfliers=False)
plt.title('Transaction Amounts by Fraud Status')
plt.suptitle('')  # Remove default title
plt.xlabel('Fraud Status (0=Legitimate, 1=Fraud)')
plt.ylabel('Transaction Amount')
plt.show()

# Visualize age distribution for fraudulent vs. legitimate transactions
plt.figure(figsize=(10, 6))
data[data['fraud'] == 0]['age'].plot(kind='density', label='Legitimate', alpha=0.7)
data[data['fraud'] == 1]['age'].plot(kind='density', label='Fraud', alpha=0.7)
plt.title('Age Distribution by Fraud Status')
plt.xlabel('Age')
plt.legend()
plt.show()

# Visualize fraud cases by country
top_countries = data['Country'].value_counts().head(10).index
fraud_by_country = data[data['Country'].isin(top_countries)].groupby('Country')['fraud'].sum()
plt.figure(figsize=(10, 6))
fraud_by_country.plot(kind='bar', color='green')
plt.title('Fraud Cases by Country (Top 10 Countries)')
plt.xlabel('Country')
plt.ylabel('Number of Fraudulent Transactions')
plt.show()












# Analyze missing values
missing_values = data.isnull().sum()
missing_percentage = (missing_values / len(data)) * 100

# Create a DataFrame to summarize missing value information
missing_summary = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage (%)': missing_percentage
}).sort_values(by='Missing Values', ascending=False)

# Display the summary in tabular format
from IPython.display import display
display(missing_summary)





# Replace missing values in 'Country' with 'Unknown'
data.fillna({'Country':'Unknown'}, inplace=True)

# Verify if missing values in 'Country' have been handled
missing_values_after = data['Country'].isnull().sum()
missing_values_after





# Copy data to handle transformations
data_encoded = data.copy()

# Encode categorical variables using label encoding or similar methods
categorical_cols = ['store', 'browser', 'sex', 'Country']
for col in categorical_cols:
    data_encoded[col] = data_encoded[col].astype('category').cat.codes

# Drop non-numeric columns that can't be directly converted to numeric
non_numeric_cols = ['signup_datetime', 'datetime', 'ip_address']
data_encoded = data_encoded.drop(columns=non_numeric_cols, errors='ignore')

# Calculate the correlation matrix
correlation_matrix = data_encoded.corr()

# Focus on correlations with the 'fraud' target variable
fraud_correlation = correlation_matrix['fraud'].sort_values(ascending=False)

# Display the results
print(fraud_correlation)








# 1. Transaction Amount vs. Fraud (Boxplot)
plt.figure(figsize=(10, 6))
data.boxplot(column='amount', by='fraud', showfliers=False)
plt.title('Transaction Amounts by Fraud Status')
plt.suptitle('')  # Remove default title
plt.xlabel('Fraud Status (0=Legitimate, 1=Fraud)')
plt.ylabel('Transaction Amount')
plt.show()

# 2. Fraud Rate by Browser
browser_fraud_rate = data.groupby('browser')['fraud'].mean().sort_values(ascending=False)
plt.figure(figsize=(10, 6))
browser_fraud_rate.plot(kind='bar', color='purple')
plt.title('Fraud Rate by Browser')
plt.xlabel('Browser')
plt.ylabel('Fraud Rate')
plt.show()

# 3. Fraud Rate by Store
store_fraud_rate = data.groupby('store')['fraud'].mean().sort_values(ascending=False)
plt.figure(figsize=(10, 6))
store_fraud_rate.plot(kind='bar', color='orange')
plt.title('Fraud Rate by Store')
plt.xlabel('Store')
plt.ylabel('Fraud Rate')
plt.show()

# 4. Fraud by Signup Time (Hour of Day)
data['signup_hour'] = pd.to_datetime(data['signup_datetime']).dt.hour
signup_hour_fraud = data.groupby('signup_hour')['fraud'].mean()
plt.figure(figsize=(10, 6))
signup_hour_fraud.plot(kind='line', marker='o')
plt.title('Fraud Rate by Signup Hour')
plt.xlabel('Signup Hour')
plt.ylabel('Fraud Rate')
plt.grid()
plt.show()

# 5. Fraud by Country
top_countries = data['Country'].value_counts().head(10).index
fraud_by_country = data[data['Country'].isin(top_countries)].groupby('Country')['fraud'].sum()
plt.figure(figsize=(10, 6))
fraud_by_country.plot(kind='bar', color='green')
plt.title('Fraud Cases by Country (Top 10 Countries)')
plt.xlabel('Country')
plt.ylabel('Number of Fraudulent Transactions')
plt.show()

# 6. Fraud by Age Group
data['age_group'] = pd.cut(data['age'], bins=[0, 18, 25, 35, 50, 100], labels=['<18', '18-25', '26-35', '36-50', '>50'])
age_group_fraud_rate = data.groupby('age_group')['fraud'].mean()
plt.figure(figsize=(10, 6))
age_group_fraud_rate.plot(kind='bar', color='blue')
plt.title('Fraud Rate by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Fraud Rate')
plt.show()








# 1. Date-Time Features
data['signup_datetime'] = pd.to_datetime(data['signup_datetime'])
data['datetime'] = pd.to_datetime(data['datetime'])

# Extracting transaction hour, day of week, and time difference
data['transaction_hour'] = data['datetime'].dt.hour
data['transaction_day'] = data['datetime'].dt.dayofweek
data['time_since_signup'] = (data['datetime'] - data['signup_datetime']).dt.total_seconds() / (3600 * 24)  # Days



from sklearn.preprocessing import StandardScaler, OneHotEncoder
from IPython.display import display


# 2. Categorical Encoding
# One-hot encode 'store' and 'browser'
encoder = OneHotEncoder(drop='first', sparse_output=False)
categorical_encoded = pd.DataFrame(
    encoder.fit_transform(data[['store', 'browser']]),
    columns=encoder.get_feature_names_out(['store', 'browser'])
)

data = pd.concat([data, categorical_encoded], axis=1)

# Frequency encoding for 'Country'
country_frequency = data['Country'].value_counts(normalize=True)
data['country_frequency'] = data['Country'].map(country_frequency)

# Encode 'sex' as numeric
data['sex'] = data['sex'].astype('category').cat.codes



# 3. Numerical Feature Transformation
scaler = StandardScaler()
data[['amount', 'age', 'time_since_signup']] = scaler.fit_transform(data[['amount', 'age', 'time_since_signup']])



# Dropping unnecessary columns for modeling
columns_to_drop = [
    'age_group', 'store', 'browser', 'Country', 'signup_datetime', 'datetime', 'ip_address'
]
data = data.drop(columns=columns_to_drop, errors='ignore')



# Handle Missing Values
if data.isnull().sum().any():
    # Fill missing numerical values with the median
    for col in data.select_dtypes(include=np.number).columns:
        data[col].fillna(data[col].median(), inplace=True)
    
    # Fill missing categorical values with the mode
    for col in data.select_dtypes(include='object').columns:
        data[col].fillna(data[col].mode()[0], inplace=True)

display(data)





from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score

# Define features (X) and target (y)
X = data.drop(columns=['fraud', 'user_id'])
y = data['fraud']

X = X.to_numpy()
y = y.to_numpy()

# Split the data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
    report = classification_report(y_test, y_pred, output_dict=True)
    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else None
    results[name] = {"classification_report": report, "roc_auc": auc}

results


# Prepare a detailed comparison table for metrics (precision, recall, F1-score, support, ROC-AUC)
metrics_comparison = []

for model_name, result in results.items():
    report = result["classification_report"]
    auc = result["roc_auc"]
    for label in report.keys():
        if label in ['0', '1']:  # Only consider the fraud (1) and non-fraud (0) classes
            metrics_comparison.append({
                "Model": model_name,
                "Class": label,
                "Precision": report[label]['precision'],
                "Recall": report[label]['recall'],
                "F1-Score": report[label]['f1-score'],
                "Support": report[label]['support'],
                "ROC-AUC": auc if label == '1' else None  # ROC-AUC is a single score, include it only for 'fraud'
            })

# Convert the metrics to a DataFrame for tabular presentation
comparison_df = pd.DataFrame(metrics_comparison)

# Display the comparison table to the user
comparison_df


import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc
import numpy as np

# Train and evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
    report = classification_report(y_test, y_pred, output_dict=True)
    auc_score = roc_auc_score(y_test, y_proba) if y_proba is not None else None
    results[name] = {"classification_report": report, "roc_auc": auc_score, "proba": y_proba}

# Prepare data for plotting
metrics = ["Precision", "Recall", "F1-Score"]
classes = ["0 (Non-Fraud)", "1 (Fraud)"]
models_list = list(models.keys())

# Aggregate metrics for each model and class
plot_data = {metric: {cls: [] for cls in classes} for metric in metrics}
for model_name, result in results.items():
    report = result["classification_report"]
    for metric in metrics:
        for cls in classes:
            cls_label = cls.split()[0]  # Extract class label ("0" or "1")
            plot_data[metric][cls].append(report[cls_label][metric.lower()])

# Create bar plots for each metric
for metric in metrics:
    x = np.arange(len(models_list))  # model indices
    width = 0.35  # bar width
    
    fig, ax = plt.subplots(figsize=(10, 6))
    for idx, cls in enumerate(classes):
        ax.bar(x + idx * width, plot_data[metric][cls], width, label=cls)
    
    # Add labels and title
    ax.set_xlabel("Models")
    ax.set_ylabel(metric)
    ax.set_title(f"{metric} Comparison Across Models")
    ax.set_xticks(x + width / 2)
    ax.set_xticklabels(models_list)
    ax.legend(title="Class")
    
    plt.show()

# Generate ROC curves for all models
plt.figure(figsize=(10, 6))
for model_name, result in results.items():
    if result["proba"] is not None:
        y_proba = result["proba"]
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{model_name} (AUC = {roc_auc:.2f})")

# Add plot details
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves")
plt.legend(loc="lower right")
plt.show()





# Evaluate training and test performance for overfitting check
overfitting_check = []

for name, model in models.items():
    y_train_pred = model.predict(X_train)
    y_train_proba = model.predict_proba(X_train)[:, 1] if hasattr(model, "predict_proba") else None
    y_test_pred = model.predict(X_test)
    y_test_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
    
    # Metrics for training and test data
    train_auc = roc_auc_score(y_train, y_train_proba) if y_train_proba is not None else None
    test_auc = roc_auc_score(y_test, y_test_proba) if y_test_proba is not None else None
    
    train_report = classification_report(y_train, y_train_pred, output_dict=True)
    test_report = classification_report(y_test, y_test_pred, output_dict=True)
    
    # Append results
    overfitting_check.append({
        "Model": name,
        "Train ROC-AUC": train_auc,
        "Test ROC-AUC": test_auc,
        "Train F1-Score (Fraud)": train_report['1']['f1-score'],
        "Test F1-Score (Fraud)": test_report['1']['f1-score']
    })

# Convert the overfitting check results to a DataFrame for easier comparison
overfitting_df = pd.DataFrame(overfitting_check)

# Display the comparison table to the user
overfitting_df





from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation for each model and calculate ROC-AUC
cv_results = []

for name, model in models.items():
    if hasattr(model, "predict_proba"):
        auc_scores = cross_val_score(
            model, X, y, cv=5, scoring='roc_auc'
        )  # Use ROC-AUC as the scoring metric
        cv_results.append({
            "Model": name,
            "Mean ROC-AUC": auc_scores.mean(),
            "Std ROC-AUC": auc_scores.std()
        })

# Convert the cross-validation results to a DataFrame for easier interpretation
cv_results_df = pd.DataFrame(cv_results)

# Display the cross-validation summary to the user
cv_results_df


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Build the Neural Network
model = Sequential([
    Dense(64, input_dim=X_train.shape[1], activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), 
              loss='binary_crossentropy', 
              metrics=['Precision', 'Recall', 'AUC'])

# Early stopping to avoid overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train, 
                    validation_split=0.2, 
                    epochs=50, 
                    batch_size=32, 
                    callbacks=[early_stopping],
                    verbose=1)

# Evaluate on test data
results = model.evaluate(X_test, y_test, verbose=1)
print("Test Loss:", results[0])
print("Test Precision:", results[1])
print("Test Recall:", results[2])
print("Test AUC:", results[3])





from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense

# Define Autoencoder
def create_autoencoder(input_dim):
    input_layer = Input(shape=(input_dim,))
    encoded = Dense(64, activation='relu')(input_layer)
    encoded = Dense(32, activation='relu')(encoded)
    decoded = Dense(64, activation='relu')(encoded)
    output_layer = Dense(input_dim, activation='sigmoid')(decoded)
    return Model(inputs=input_layer, outputs=output_layer)

# Compile and train Autoencoder
autoencoder = create_autoencoder(X_train.shape[1])
autoencoder.compile(optimizer='adam', loss='mse')

autoencoder.fit(X_train, X_train, 
                epochs=50, 
                batch_size=32, 
                validation_split=0.2, 
                verbose=1)

# Use reconstruction error for anomaly detection

reconstruction = autoencoder.predict(X_test)

# Calculate reconstruction error
reconstruction_error = ((X_test - reconstruction) ** 2).mean(axis=1)

# Convert reconstruction error to a Pandas Series
reconstruction_error_series = pd.Series(reconstruction_error)

# Set the threshold based on the 95th percentile
threshold = reconstruction_error_series.quantile(0.95)

# Predict anomalies (frauds)
y_pred = (reconstruction_error > threshold).astype(int)



from sklearn.metrics import classification_report, roc_auc_score

# Evaluate performance
print("Threshold for anomaly detection:", threshold)
print()

# Classification report
report = classification_report(y_test, y_pred, target_names=["Not Fraud", "Fraud"])
print("Classification Report:\n", report)

# ROC-AUC
auc_score = roc_auc_score(y_test, reconstruction_error)
print("ROC-AUC Score:", auc_score)



import matplotlib.pyplot as plt

# Plot distribution of reconstruction error
plt.figure(figsize=(10, 6))
plt.hist(reconstruction_error[y_test == 0], bins=50, alpha=0.6, label="Not Fraud")
plt.hist(reconstruction_error[y_test == 1], bins=50, alpha=0.6, label="Fraud")
plt.axvline(threshold, color='r', linestyle='--', label='Threshold')
plt.title("Reconstruction Error Distribution")
plt.xlabel("Reconstruction Error")
plt.ylabel("Frequency")
plt.legend()
plt.show()



from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Not Fraud", "Fraud"])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()



from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Define the MLP model
def create_mlp(input_dim):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification
    return model

# Compile and train the model
input_dim = X_train.shape[1]
mlp_model = create_mlp(input_dim)
mlp_model.compile(optimizer=Adam(learning_rate=0.001), 
                  loss='binary_crossentropy', 
                  metrics=['Precision', 'Recall', 'AUC'])

early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)

history = mlp_model.fit(X_train, y_train, 
                        validation_split=0.2, 
                        epochs=50, 
                        batch_size=32, 
                        callbacks=[early_stopping])



# Evaluate the model on the test set
results = mlp_model.evaluate(X_test, y_test, verbose=1)
print("Test Loss:", results[0])
print("Test Precision:", results[1])
print("Test Recall:", results[2])
print("Test AUC:", results[3])



# Define the model-building function
def build_model(hp):
    model = Sequential()
    # Input layer
    model.add(Dense(units=hp.Int('units_input', min_value=32, max_value=256, step=32), 
                    activation='relu', input_dim=X_train.shape[1]))
    # Hidden layers
    for i in range(hp.Int('num_hidden_layers', 1, 3)):
        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32), activation='relu'))
        model.add(Dropout(hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))
    # Output layer
    model.add(Dense(1, activation='sigmoid'))
    # Compile the model
    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),
                  loss='binary_crossentropy',
                  metrics=['Precision', 'Recall', 'AUC'])
    return model


from keras_tuner.tuners import RandomSearch

# Set up the tuner
tuner = RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=10,  # Number of hyperparameter combinations to try
    executions_per_trial=2,  # Average performance over 2 runs
    directory='hyperparam_tuning',
    project_name='fraud_detection'
)

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)

# Run the tuner
tuner.search(X_train, y_train, 
             validation_split=0.2, 
             epochs=50, 
             batch_size=32, 
             callbacks=[early_stopping])



# Retrieve the best hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print("Best Hyperparameters:")
print(best_hps.values)



# Build the model with the best hyperparameters
best_model = tuner.hypermodel.build(best_hps)

# Train the model
history = best_model.fit(X_train, y_train, 
                         validation_split=0.2, 
                         epochs=50, 
                         batch_size=32, 
                         callbacks=[early_stopping])



# Evaluate the best model on the test set
results = best_model.evaluate(X_test, y_test, verbose=1)
print("Test Loss:", results[0])
print("Test Precision:", results[1])
print("Test Recall:", results[2])
print("Test AUC:", results[3])




