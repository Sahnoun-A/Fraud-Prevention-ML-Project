import pandas as pd

# Load the uploaded CSV files
txns_df = pd.read_csv('ecom_txns.csv')
ip_mappings_df = pd.read_csv('ip_mappings.csv')

# Display the first few rows of each dataset to inspect the data
txns_df.head(), ip_mappings_df.head()






import ipaddress
from bisect import bisect_left

# Clean IP ranges to ensure they are valid IPv4 addresses
def valid_ip(ip):
    try:
        ipaddress.IPv4Address(ip)
        return True
    except ipaddress.AddressValueError:
        return False

# Filter out invalid IP ranges in ip_mappings_df
ip_mappings_df = ip_mappings_df[ip_mappings_df['range_start'].apply(valid_ip) & ip_mappings_df['range_end'].apply(valid_ip)]

# Function to convert IP to integer
def ip_to_int(ip):
    return int(ipaddress.IPv4Address(ip))

# Convert the IP ranges to integer values for efficient comparison
ip_mappings_df['range_start_int'] = ip_mappings_df['range_start'].apply(ip_to_int)
ip_mappings_df['range_end_int'] = ip_mappings_df['range_end'].apply(ip_to_int)

# Sort IP mappings by the start IP for faster lookup
ip_mappings_df = ip_mappings_df.sort_values(by='range_start_int')

# Create lists of start and end IP integers for fast comparison
range_starts = ip_mappings_df['range_start_int'].values
range_ends = ip_mappings_df['range_end_int'].values
countries = ip_mappings_df['country'].values

# Efficiently map IP to country using binary search
def fast_map_ip_to_country(ip, range_starts, range_ends, countries):
    ip_int = ip_to_int(ip)
    # Use binary search to find the position where this IP could fit in the sorted ranges
    idx = bisect_left(range_starts, ip_int)
    if idx < len(range_starts) and ip_int >= range_starts[idx] and ip_int <= range_ends[idx]:
        return countries[idx]
    return None

# Apply the optimized mapping function
txns_df['country'] = txns_df['ip_address'].apply(fast_map_ip_to_country, 
                                                  range_starts=range_starts, 
                                                  range_ends=range_ends, 
                                                  countries=countries)

# Display the updated dataframe with the country column
txns_df.head()



txns_df['country'].values


from IPython.display import FileLink

# Save the DataFrame as a CSV file
txns_df.to_csv('merged_file.csv', index=False)

# Create a link to download the file
FileLink(r'merged_file.csv')





print(txns_df.columns)


# Handle missing values
# We can impute missing values for 'sex' and 'country' with 'Unknown' or most frequent values
txns_df['sex'] = txns_df['sex'].fillna('Unknown')
txns_df['country'] = txns_df['country'].fillna('Unknown')

# Handle missing age (we can impute with the median age, or you could choose another method)
txns_df['age'] = txns_df['age'].fillna(txns_df['age'].median())

# Convert datetime columns to datetime type
txns_df['signup_datetime'] = pd.to_datetime(txns_df['signup_datetime'])
txns_df['datetime'] = pd.to_datetime(txns_df['datetime'])

# Feature Engineering: Account age in days
txns_df['account_age'] = (txns_df['datetime'] - txns_df['signup_datetime']).dt.days

# Extract hour of transaction (time-based feature)
txns_df['transaction_hour'] = txns_df['datetime'].dt.hour

# Encode categorical variables using Label Encoding
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
txns_df['sex'] = encoder.fit_transform(txns_df['sex'])
txns_df['store'] = encoder.fit_transform(txns_df['store'])
txns_df['browser'] = encoder.fit_transform(txns_df['browser'])
txns_df['country'] = encoder.fit_transform(txns_df['country'])

# Drop unnecessary columns
txns_df = txns_df.drop(['signup_datetime', 'datetime', 'device_id', 'ip_address'], axis=1)

txns_df.head()



!conda install -c conda-forge xgboost -y


from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Features and target variable
X = txns_df.drop(['user_id', 'fraud'], axis=1)  # Drop user_id and target column 'fraud'
y = txns_df['fraud']

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define models to be tested
models = {
    "Random Forest": RandomForestClassifier(class_weight='balanced', random_state=42),
    "Logistic Regression": LogisticRegression(class_weight='balanced', random_state=42),
    "Support Vector Machine (SVM)": SVC(class_weight='balanced', random_state=42),
    "K-Nearest Neighbors (KNN)": KNeighborsClassifier(),
    "XGBoost": XGBClassifier(scale_pos_weight=9)  # Adjusted scale_pos_weight for imbalance
}

# Function to evaluate models
def evaluate_model(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return classification_report(y_test, y_pred), confusion_matrix(y_test, y_pred)

# Evaluate each model and store the results
results = {}
for name, model in models.items():
    classification_rep, conf_matrix = evaluate_model(model, X_train, X_test, y_train, y_test)
    results[name] = {
        "classification_report": classification_rep,
        "confusion_matrix": conf_matrix
    }

results




