














import pandas as pd
import numpy as np

import warnings
warnings.filterwarnings("ignore")

# Load the CSV file
data = pd.read_csv('ecom_txns.csv')

# Display the first few rows of the dataset
data.head()


data.info()





import geoip2.database


# Initialize the reader
reader = geoip2.database.Reader('GeoLite2-Country.mmdb')

# Function to fetch country from IP address
def get_country_from_ip(ip):
    try:
        response = reader.country(ip)
        return response.country.name
    except:
        return "Unknown"

# Apply the function to the dataset
data['Country'] = data['ip_address'].apply(get_country_from_ip)

# Close the reader
reader.close()



# Display the updated dataset
data.head()


from IPython.display import FileLink

# Save the DataFrame as a CSV file
data.to_csv('updated_file.csv', index=False)

# Create a link to download the file
FileLink(r'updated_file.csv')





import matplotlib.pyplot as plt

# Visualize the fraud distribution
fraud_counts = data['fraud'].value_counts()
plt.figure(figsize=(6, 4))
fraud_counts.plot(kind='bar', color=['blue', 'orange'])
plt.title('Fraud Distribution')
plt.xticks(ticks=[0, 1], labels=['Legitimate', 'Fraud'])
plt.ylabel('Count')
plt.xlabel('Transaction Type')
plt.show()

# Visualize transaction amounts for fraudulent vs. legitimate transactions
plt.figure(figsize=(10, 6))
data.boxplot(column='amount', by='fraud', showfliers=False)
plt.title('Transaction Amounts by Fraud Status')
plt.suptitle('')  # Remove default title
plt.xlabel('Fraud Status (0=Legitimate, 1=Fraud)')
plt.ylabel('Transaction Amount')
plt.show()

# Visualize age distribution for fraudulent vs. legitimate transactions
plt.figure(figsize=(10, 6))
data[data['fraud'] == 0]['age'].plot(kind='density', label='Legitimate', alpha=0.7)
data[data['fraud'] == 1]['age'].plot(kind='density', label='Fraud', alpha=0.7)
plt.title('Age Distribution by Fraud Status')
plt.xlabel('Age')
plt.legend()
plt.show()

# Visualize fraud cases by country
top_countries = data['Country'].value_counts().head(10).index
fraud_by_country = data[data['Country'].isin(top_countries)].groupby('Country')['fraud'].sum()
plt.figure(figsize=(10, 6))
fraud_by_country.plot(kind='bar', color='green')
plt.title('Fraud Cases by Country (Top 10 Countries)')
plt.xlabel('Country')
plt.ylabel('Number of Fraudulent Transactions')
plt.show()












# Analyze missing values
missing_values = data.isnull().sum()
missing_percentage = (missing_values / len(data)) * 100

# Create a DataFrame to summarize missing value information
missing_summary = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage (%)': missing_percentage
}).sort_values(by='Missing Values', ascending=False)

# Display the summary in tabular format
from IPython.display import display
display(missing_summary)





# Replace missing values in 'Country' with 'Unknown'
data.fillna({'Country':'Unknown'}, inplace=True)

# Verify if missing values in 'Country' have been handled
missing_values_after = data['Country'].isnull().sum()
missing_values_after





# Copy data to handle transformations
data_encoded = data.copy()

# Encode categorical variables using label encoding
categorical_cols = ['store', 'browser', 'sex', 'Country']
for col in categorical_cols:
    data_encoded[col] = data_encoded[col].astype('category').cat.codes

# Drop non-numeric columns that can't be directly converted to numeric
non_numeric_cols = ['signup_datetime', 'datetime', 'ip_address']
data_encoded = data_encoded.drop(columns=non_numeric_cols, errors='ignore')

# Calculate the correlation matrix
correlation_matrix = data_encoded.corr()

# Focus on correlations with the 'fraud' target variable
fraud_correlation = correlation_matrix['fraud'].sort_values(ascending=False)

# Display the results
print(fraud_correlation)








# 1. Transaction Amount vs. Fraud (Boxplot)
plt.figure(figsize=(10, 6))
data.boxplot(column='amount', by='fraud', showfliers=False)
plt.title('Transaction Amounts by Fraud Status')
plt.suptitle('')  # Remove default title
plt.xlabel('Fraud Status (0=Legitimate, 1=Fraud)')
plt.ylabel('Transaction Amount')
plt.show()

# 2. Fraud Rate by Browser
browser_fraud_rate = data.groupby('browser')['fraud'].mean().sort_values(ascending=False)
plt.figure(figsize=(10, 6))
browser_fraud_rate.plot(kind='bar', color='purple')
plt.title('Fraud Rate by Browser')
plt.xlabel('Browser')
plt.ylabel('Fraud Rate')
plt.show()

# 3. Fraud Rate by Store
store_fraud_rate = data.groupby('store')['fraud'].mean().sort_values(ascending=False)
plt.figure(figsize=(10, 6))
store_fraud_rate.plot(kind='bar', color='orange')
plt.title('Fraud Rate by Store')
plt.xlabel('Store')
plt.ylabel('Fraud Rate')
plt.show()

# 4. Fraud by Signup Time (Hour of Day)
data['signup_hour'] = pd.to_datetime(data['signup_datetime']).dt.hour
signup_hour_fraud = data.groupby('signup_hour')['fraud'].mean()
plt.figure(figsize=(10, 6))
signup_hour_fraud.plot(kind='line', marker='o')
plt.title('Fraud Rate by Signup Hour')
plt.xlabel('Signup Hour')
plt.ylabel('Fraud Rate')
plt.grid()
plt.show()

# 5. Fraud by Country
top_countries = data['Country'].value_counts().head(10).index
fraud_by_country = data[data['Country'].isin(top_countries)].groupby('Country')['fraud'].sum()
plt.figure(figsize=(10, 6))
fraud_by_country.plot(kind='bar', color='green')
plt.title('Fraud Cases by Country (Top 10 Countries)')
plt.xlabel('Country')
plt.ylabel('Number of Fraudulent Transactions')
plt.show()

# 6. Fraud by Age Group
data['age_group'] = pd.cut(data['age'], bins=[0, 18, 25, 35, 50, 100], labels=['<18', '18-25', '26-35', '36-50', '>50'])
age_group_fraud_rate = data.groupby('age_group')['fraud'].mean()
plt.figure(figsize=(10, 6))
age_group_fraud_rate.plot(kind='bar', color='blue')
plt.title('Fraud Rate by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Fraud Rate')
plt.show()








# 1. Date-Time Features
data['signup_datetime'] = pd.to_datetime(data['signup_datetime'])
data['datetime'] = pd.to_datetime(data['datetime'])

# Extracting transaction hour, day of week, and time difference
data['transaction_hour'] = data['datetime'].dt.hour
data['transaction_day'] = data['datetime'].dt.dayofweek
data['time_since_signup'] = (data['datetime'] - data['signup_datetime']).dt.total_seconds() / (3600 * 24)  # Days



from sklearn.preprocessing import StandardScaler, OneHotEncoder
from IPython.display import display
import pickle


# 2. Categorical Encoding
# One-hot encode 'store' and 'browser'
encoder = OneHotEncoder(drop='first', sparse_output=False)
categorical_encoded = pd.DataFrame(
    encoder.fit_transform(data[['store', 'browser']]),
    columns=encoder.get_feature_names_out(['store', 'browser'])
)

data = pd.concat([data, categorical_encoded], axis=1)

# save the encoder
with open('encoder.pkl', 'wb') as f:
    pickle.dump(encoder, f)

# Frequency encoding for 'Country'
country_frequency = data['Country'].value_counts(normalize=True)
data['country_frequency'] = data['Country'].map(country_frequency)

# Save the frequency encoding
with open('country_freq_encoding.pkl', 'wb') as f:
    pickle.dump(country_frequency, f)

# Encode 'sex' as numeric
sex_encoding = {'M': 0, 'F': 1, 'Not Provided': 2}

data['sex'] = data['sex'].map(sex_encoding)

# Save the encoding
with open('sex_encoding.pkl', 'wb') as f:
    pickle.dump(sex_encoding, f)


# 3. Numerical Feature Transformation
scaler = StandardScaler()
data[['amount', 'age', 'time_since_signup']] = scaler.fit_transform(data[['amount', 'age', 'time_since_signup']])

# Save the scaler
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)


# Dropping unnecessary columns for modeling
columns_to_drop = [
    'age_group', 'store', 'browser', 'Country', 'signup_datetime', 'datetime', 'ip_address'
]
data = data.drop(columns=columns_to_drop, errors='ignore')



# Handle Missing Values
if data.isnull().sum().any():
    # Fill missing numerical values with the median
    for col in data.select_dtypes(include=np.number).columns:
        data[col].fillna(data[col].median(), inplace=True)
    
    # Fill missing categorical values with the mode
    for col in data.select_dtypes(include='object').columns:
        data[col].fillna(data[col].mode()[0], inplace=True)

display(data)





from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score

# Define features (X) and target (y)
X = data.drop(columns=['fraud', 'user_id'])
y = data['fraud']

X = X.to_numpy()
y = y.to_numpy()

# Split the data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
    report = classification_report(y_test, y_pred, output_dict=True)
    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else None
    results[name] = {"classification_report": report, "roc_auc": auc}

results


# Prepare a detailed comparison table for metrics (precision, recall, F1-score, support, ROC-AUC)
metrics_comparison = []

for model_name, result in results.items():
    report = result["classification_report"]
    auc = result["roc_auc"]
    for label in report.keys():
        if label in ['0', '1']:  # consider both the fraud (1) and non-fraud (0) classes
            metrics_comparison.append({
                "Model": model_name,
                "Class": label,
                "Precision": report[label]['precision'],
                "Recall": report[label]['recall'],
                "F1-Score": report[label]['f1-score'],
                "Support": report[label]['support'],
                "ROC-AUC": auc if label == '1' else None  # ROC-AUC is a single score, include it only for 'fraud'
            })

# Convert the metrics to a DataFrame for tabular presentation
comparison_df = pd.DataFrame(metrics_comparison)

# Display the comparison table to the user
comparison_df


import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Train and evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
    report = classification_report(y_test, y_pred, output_dict=True)
    auc_score = roc_auc_score(y_test, y_proba) if y_proba is not None else None
    results[name] = {"classification_report": report, "roc_auc": auc_score, "proba": y_proba}

# Prepare data for plotting
metrics = ["Precision", "Recall", "F1-Score"]
classes = ["0 (Non-Fraud)", "1 (Fraud)"]
models_list = list(models.keys())

# Aggregate metrics for each model and class
plot_data = {metric: {cls: [] for cls in classes} for metric in metrics}
for model_name, result in results.items():
    report = result["classification_report"]
    for metric in metrics:
        for cls in classes:
            cls_label = cls.split()[0]  # Extract class label ("0" or "1")
            plot_data[metric][cls].append(report[cls_label][metric.lower()])

# Create bar plots for each metric
for metric in metrics:
    x = np.arange(len(models_list))  # model indices
    width = 0.35  # bar width
    
    fig, ax = plt.subplots(figsize=(10, 6))
    for idx, cls in enumerate(classes):
        ax.bar(x + idx * width, plot_data[metric][cls], width, label=cls)
    
    # Add labels and title
    ax.set_xlabel("Models")
    ax.set_ylabel(metric)
    ax.set_title(f"{metric} Comparison Across Models")
    ax.set_xticks(x + width / 2)
    ax.set_xticklabels(models_list)
    ax.legend(title="Class")
    
    plt.show()

# Generate ROC curves for all models
plt.figure(figsize=(10, 6))
for model_name, result in results.items():
    if result["proba"] is not None:
        y_proba = result["proba"]
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{model_name} (AUC = {roc_auc:.2f})")

# Add plot details
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves")
plt.legend(loc="lower right")
plt.show()








# Evaluate training and test performance for overfitting check
overfitting_check = []

for name, model in models.items():
    y_train_pred = model.predict(X_train)
    y_train_proba = model.predict_proba(X_train)[:, 1] if hasattr(model, "predict_proba") else None
    y_test_pred = model.predict(X_test)
    y_test_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
    
    # Metrics for training and test data
    train_auc = roc_auc_score(y_train, y_train_proba) if y_train_proba is not None else None
    test_auc = roc_auc_score(y_test, y_test_proba) if y_test_proba is not None else None
    
    train_report = classification_report(y_train, y_train_pred, output_dict=True)
    test_report = classification_report(y_test, y_test_pred, output_dict=True)
    
    # Append results
    overfitting_check.append({
        "Model": name,
        "Train ROC-AUC": train_auc,
        "Test ROC-AUC": test_auc,
        "Train F1-Score (Fraud)": train_report['1']['f1-score'],
        "Test F1-Score (Fraud)": test_report['1']['f1-score']
    })

# Convert the overfitting check results to a DataFrame for easier comparison
overfitting_df = pd.DataFrame(overfitting_check)

# Display the comparison table to the user
overfitting_df





from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation for each model and calculate ROC-AUC
cv_results = []

for name, model in models.items():
    if hasattr(model, "predict_proba"):
        auc_scores = cross_val_score(
            model, X, y, cv=5, scoring='roc_auc'
        )  # Use ROC-AUC as the scoring metric
        cv_results.append({
            "Model": name,
            "Mean ROC-AUC": auc_scores.mean(),
            "Std ROC-AUC": auc_scores.std()
        })

# Convert the cross-validation results to a DataFrame for easier interpretation
cv_results_df = pd.DataFrame(cv_results)

# Display the cross-validation summary
cv_results_df






from sklearn.model_selection import train_test_split, GridSearchCV

# Define hyperparameter grids for both models
rf_param_grid = {
    "n_estimators": [50, 100, 200],
    "max_depth": [10, 20, 30],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
}

xgb_param_grid = {
    "n_estimators": [50, 100, 200],
    "max_depth": [3, 6, 10],
    "learning_rate": [0.01, 0.1, 0.2],
    "subsample": [0.6, 0.8, 1.0],
}

# Random Forest Grid Search
rf_grid_search = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=rf_param_grid,
    scoring="roc_auc",
    cv=3,
    n_jobs=-1,          # Uses all CPU cores to speed up computation
    verbose=1,          # Displays progress output
)
rf_grid_search.fit(X_train, y_train)

# XGBoost Grid Search
xgb_grid_search = GridSearchCV(
    estimator=XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42),
    param_grid=xgb_param_grid,
    scoring="roc_auc",
    cv=3,
    n_jobs=-1,          # Uses all CPU cores to speed up computation
    verbose=1,          # Displays progress output
)
xgb_grid_search.fit(X_train, y_train)

# Best parameters and scores
rf_best_params = rf_grid_search.best_params_
rf_best_score = rf_grid_search.best_score_

xgb_best_params = xgb_grid_search.best_params_
xgb_best_score = xgb_grid_search.best_score_

# Display the results
print("Random Forest Best Parameters:", rf_best_params)
print("Random Forest Best ROC-AUC (CV):", rf_best_score)

print("XGBoost Best Parameters:", xgb_best_params)
print("XGBoost Best ROC-AUC (CV):", xgb_best_score)




# After the models have been fine-tuned and refit with the best parameters

# Random Forest
rf_model = RandomForestClassifier(**rf_best_params, random_state=42)

rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)
# XGBoost
xgb_model = XGBClassifier(**xgb_best_params, use_label_encoder=False, eval_metric="logloss", random_state=42)

xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_test)





import joblib

# Save the Random Forest model
joblib.dump(rf_model, 'random_forest_model.pkl')

# Save the XGBoost model
joblib.dump(xgb_model, 'xgboost_model.pkl')



